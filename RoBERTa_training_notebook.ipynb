{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yMa-OLFKidua"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjfUfKu2JrTo"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HWslNrUJdla"},"outputs":[],"source":["import os\n","import re\n","import random\n","import pandas as pd\n","import numpy as np\n","import csv\n","import tensorflow as tf\n","import torch\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import textwrap\n","import progressbar\n","import keras\n","from keras_preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","import time\n","import datetime\n","import json\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","    \n","def metrics_calculator(preds, test_labels):\n","    cm = confusion_matrix(test_labels, preds)\n","    TP = []\n","    FP = []\n","    FN = []\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[i][j]\n","\n","        FN.append(summ)\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[j][i]\n","\n","        FP.append(summ)\n","    for i in range(0,2):\n","        TP.append(cm[i][i])\n","    precision = []\n","    recall = []\n","    for i in range(0,2):\n","        precision.append(TP[i]/(TP[i] + FP[i]))\n","        recall.append(TP[i]/(TP[i] + FN[i]))\n","\n","    macro_precision = sum(precision)/2\n","    macro_recall = sum(recall)/2\n","    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n","    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n","    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n","    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n","    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n","\n","seed_val = 2212\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7Q9PNnweVuf"},"outputs":[],"source":["# load all models and select roberta\n","from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n","from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n","from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n","from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n","from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n","from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n","\n","MODEL_CLASSES = {\n","    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n","    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n","    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n","    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n","    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n","\n","model_type = 'roberta' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n","model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n","model_name = 'roberta-base'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxShnTP4islX"},"outputs":[],"source":["#save the trained model\n","\n","#output_dir     = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Models/finetuned/roberta_trained_on_single_10eps/'\n","#pretrained_dir = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Models/finetuned/roberta_trained_on_single_preprocessed_2_10eps/'\n","pretrained_dir = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Models/finetuned/roberta_trained_on_single_10eps/'\n","\n","#input_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_train_dev_preprocessed_2.csv'\n","input_file = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_train_dev.csv'\n","\n","#test_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preprocessed_2.csv'\n","#pred_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preprocessed_2_preds.csv'\n","test_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation.csv'\n","pred_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_preds.csv'\n","\n","#test_exp_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation.csv'\n","#pred_exp_file  = '/content/drive/MyDrive/Colab Notebooks/SemEval2023/CJPE-main/Data/trainData/ILDC_single_test_explanation_pred.csv'\n","\n"]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","config = RobertaConfig.from_pretrained('roberta-base', output_hidden_states=True, output_attentions=True, num_labels=2) \n","tokenizer = tokenizer_class.from_pretrained(model_name)\n","\n","if pretrained_dir == None:\n","  print('\\n\\nno pretrained model')  \n","  model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",  config=config)\n","else:\n","  model = RobertaForSequenceClassification.from_pretrained(pretrained_dir,  config=config)\n","  print('\\n\\nload from ', pretrained_dir)\n","\n","model.to(device)"],"metadata":{"id":"4IaMjXbWvbCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBjxqwueJ0pq"},"outputs":[],"source":["#load data\n","df = pd.read_csv(input_file) # path to multi_dataset\n","df = df.applymap(lambda x: x.strip().replace('-\\n', '').replace('\\n', '') if isinstance(x, str) else x)\n","\n","#assert all(not '\\n' in df.iloc[i]['text'] for i in range(len(df)))\n","\n","train_set = df.query(\" split=='train' \")\n","validation_set = df.query(\" split=='dev' \")\n","#validation_set = pd.read_csv(test_file)\n","#test_set = pd.read_csv(test_file)"]},{"cell_type":"code","source":["validation_set['id'] = range(len(validation_set))\n","print(validation_set)"],"metadata":{"id":"W8DoEiiw5gNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4rRfixgwOnw"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"pmfUxc93wOyR"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zANK4YJ0LZEf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_Gr42aW0DQBe"},"source":["# Preparing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqBkaIbDKMlX"},"outputs":[],"source":["def input_id_maker(dataf, tokenizer):\n","  input_ids = []\n","  lengths = []\n","\n","  for i in progressbar.progressbar(range(len(dataf['text']))):\n","    sen = dataf['text'].iloc[i]\n","    sen = tokenizer.tokenize(sen, add_prefix_space=True)\n","    CLS = tokenizer.cls_token\n","    SEP = tokenizer.sep_token\n","    if(len(sen) > 510):\n","      sen = sen[len(sen)-510:]\n","    \n","    '''if len(sen) > 510:\n","      sen = sen[0:255] + sen[len(sen)-255:]'''\n","\n","    sen = [CLS] + sen + [SEP]\n","    encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n","    input_ids.append(encoded_sent)\n","    lengths.append(len(encoded_sent))\n","\n","  input_ids = pad_sequences(input_ids, maxlen=512, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n","  return input_ids, lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctB5AycbKVj6"},"outputs":[],"source":["train_input_ids, train_lengths = input_id_maker(train_set, tokenizer)\n","validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_rrz5-vKX_x"},"outputs":[],"source":["def att_masking(input_ids):\n","  attention_masks = []\n","  for sent in input_ids:\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    attention_masks.append(att_mask)\n","  return attention_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDFGL8XPKZ86"},"outputs":[],"source":["train_attention_masks = att_masking(train_input_ids)\n","validation_attention_masks = att_masking(validation_input_ids)\n","\n","train_labels = train_set['label'].to_numpy().astype('int')\n","validation_labels = validation_set['label'].to_numpy().astype('int')\n","#validation_labels = validation_set['id'].to_numpy().astype('int')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1UJt0WJKfAR"},"outputs":[],"source":["train_inputs = train_input_ids\n","validation_inputs = validation_input_ids\n","train_masks = train_attention_masks\n","validation_masks = validation_attention_masks\n","\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","source":["batch_size = 6\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)"],"metadata":{"id":"fNCXSakabP_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OK2uGb9qypeO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1-7D6UxDKPv"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyr_rKJDKqX1"},"outputs":[],"source":["'''\n","# max batch size should be 6 due to colab limits\n","batch_size = 6\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n","\n","lr = 2e-6\n","max_grad_norm = 1.0\n","epochs = 10\n","num_total_steps = len(train_dataloader)*epochs\n","num_warmup_steps = 1000\n","warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n","optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\n","\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_loss = 0\n","\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        if step % 40 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n","\n","        \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        \n","        loss = outputs[0]\n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","    if (epoch_i+1)%10 == 0:\n","    # Create output directory if needed\n","      if not os.path.exists(output_dir):\n","          os.makedirs(output_dir)\n","\n","      print(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\n","      model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","      model_to_save.save_pretrained(output_dir)\n","      tokenizer.save_pretrained(output_dir)\n","\n","\n","    if (epoch_i+1)%5 == 0:\n","        \n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      model.eval()\n","\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","\n","      for batch in validation_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          with torch.no_grad():        \n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","      \n","          logits = outputs[0]\n","\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          eval_accuracy += tmp_eval_accuracy\n","\n","          nb_eval_steps += 1\n","\n","      # Report the final accuracy for this validation run.\n","      print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","print(\"Saving model to %s (epoch %d)\" % (output_dir, epoch_i+1))\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","\n","'''"]},{"cell_type":"code","source":[],"metadata":{"id":"PpacOMWKW8H-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZL4jYL_vDEej"},"source":["#Predicting"]},{"cell_type":"markdown","metadata":{"id":"1-nrvVp0o7fy"},"source":["## Evaluation on training set\n","\n","Predicting labels for 4,982 test sentences...\n","0.9207145724608591\n","0.925262014040807 0.9064129216309111 0.9157404833006237 0.9207145724608591 0.9207145724608591 0.9207145724608591\n","    DONE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KCOktf8SDb5L"},"outputs":[],"source":["'''batch_size = 6\n","\n","prediction_data = TensorDataset(train_inputs, train_masks, train_labels)\n","prediction_sampler = SequentialSampler(train_data)\n","prediction_dataloader = DataLoader(train_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","      logits = outputs.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print(flat_accuracy(predictions,true_labels))\n","\n","macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n","print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n","\n","print('    DONE.')'''"]},{"cell_type":"markdown","metadata":{"id":"_oWVq6yHo_nC"},"source":["##evaluation on dev set\n","\n","Predicting labels for 994 test sentences...\n","0.6056338028169014\n","0.6907555653032131 0.6056338028169015 0.6454001091325866 0.6056338028169014 0.6056338028169014 0.6056338028169014\n","    DONE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nn0CrsHYM9fz"},"outputs":[],"source":["'''batch_size = 1\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","      logits = outputs.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  attentions = attentions.detach().cpu().numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print(flat_accuracy(predictions,true_labels))\n","\n","macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n","print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n","\n","print('    DONE.')'''"]},{"cell_type":"code","source":[],"metadata":{"id":"cx5vgBlrZC6l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction on binary classification test set"],"metadata":{"id":"nM5efG4mzU5G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAdWPBXvwydo"},"outputs":[],"source":["'''batch_size = 1\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","      logits = outputs.logits\n","      \n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","\n","print('    DONE.')\n","\n","validation_set['id2'] = labels_flat\n","validation_set['prediction'] = pred_flat\n","assert all(validation_set['id2'] == validation_set['id'])\n","validation_set.to_csv(pred_file, index=False,)'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"By4GNgg1_k1Q"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["##Prediction on explanation test set"],"metadata":{"id":"FEmiKq3fJ2Ja"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQbvOT4kkPxz"},"outputs":[],"source":["batch_size = 1\n","window_size = 128\n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, shuffle=False, sampler=None, batch_size = batch_size)\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","model.eval()\n","\n","predictions , true_labels = [], []\n","explanations = []\n","\n","for (step, batch) in enumerate(prediction_dataloader):\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  b_input_ids, b_input_mask, b_labels = batch\n","  #print(b_input_ids)\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","      logits = outputs.logits\n","      attentions = outputs.attentions[-1]\n","      #max_ind = attentions[0][-1][0].detach().cpu().numpy().argmax()\n","\n","      max_inds = np.argpartition(attentions[0][-1][0].detach().cpu().numpy(), -3)[-3:]\n","      \n","      explanation_sents = []\n","      for max_ind in max_inds:\n","        selected_tokens = b_input_ids[0][max_ind-int(window_size/2):max_ind+int(window_size/2)]\n","        tokens = tokenizer.convert_ids_to_tokens(selected_tokens) \n","        #print(tokens)\n","        sent = tokenizer.convert_tokens_to_string(tokens)\n","        #print(sent)\n","        explanation_sents.append(sent)\n","\n","      explanation_sents = \" \".join(explanation_sents)\n","      explanations.append(explanation_sents) \n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  attentions = attentions.detach().cpu().numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)\n","pred_flat = np.argmax(predictions, axis=1).flatten()\n","labels_flat = true_labels.flatten()\n","\n","print('    DONE.')\n","\n","\n","decisions = ['Accepted' if _ == 1 else 'Denied' for _ in pred_flat]\n","\n","validation_set['id2'] = labels_flat\n","validation_set['prediction'] = pred_flat\n","validation_set['decision'] = decisions\n","validation_set['explanation'] = explanations\n","assert all(validation_set['id2'] == validation_set['id'])\n","validation_set.to_csv(pred_file, index=False, columns=['uid','decision','explanation'] )"]},{"cell_type":"code","source":[],"metadata":{"id":"7FHOxJ_eJjAF","executionInfo":{"status":"ok","timestamp":1675526289452,"user_tz":0,"elapsed":216,"user":{"displayName":"Thanet Markchom","userId":"00244922274630544136"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZCD4UIeyJjEc"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}